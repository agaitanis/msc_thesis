I0913 06:02:48.484912 140486677665600 train.py:65] Reading the config file.
I0913 06:02:48.486569 140486677665600 train.py:69] Starting the experiment.
2022-09-13 06:02:48.486964: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-09-13 06:02:48.889031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11416 MB memory:  -> device: 0, name: NVIDIA TITAN Xp, pci bus id: 0000:65:00.0, compute capability: 6.1
I0913 06:02:48.890753 140486677665600 train_lib.py:108] Using strategy <class 'tensorflow.python.distribute.one_device_strategy.OneDeviceStrategy'> with 1 replicas
I0913 06:02:49.016706 140486677665600 deeplab.py:57] Synchronized Batchnorm is used.
I0913 06:02:49.017744 140486677665600 axial_resnet_instances.py:144] Axial-ResNet final config: {'num_blocks': [3, 4, 6, 3], 'backbone_layer_multiplier': 1.0, 'width_multiplier': 1.0, 'stem_width_multiplier': 1.0, 'output_stride': 16, 'classification_mode': True, 'backbone_type': 'resnet', 'use_axial_beyond_stride': 0, 'backbone_use_transformer_beyond_stride': 0, 'extra_decoder_use_transformer_beyond_stride': 32, 'backbone_decoder_num_stacks': 0, 'backbone_decoder_blocks_per_stage': 1, 'extra_decoder_num_stacks': 0, 'extra_decoder_blocks_per_stage': 1, 'max_num_mask_slots': 128, 'num_mask_slots': 128, 'memory_channels': 256, 'base_transformer_expansion': 1.0, 'global_feed_forward_network_channels': 256, 'high_resolution_output_stride': 4, 'activation': 'relu', 'block_group_config': {'attention_bottleneck_expansion': 2, 'drop_path_keep_prob': 1.0, 'drop_path_beyond_stride': 16, 'drop_path_schedule': 'constant', 'positional_encoding_type': None, 'use_global_beyond_stride': 0, 'use_sac_beyond_stride': -1, 'use_squeeze_and_excite': False, 'conv_use_recompute_grad': False, 'axial_use_recompute_grad': True, 'recompute_within_stride': 0, 'transformer_use_recompute_grad': False, 'axial_layer_config': {'query_shape': (129, 129), 'key_expansion': 1, 'value_expansion': 2, 'memory_flange': (32, 32), 'double_global_attention': False, 'num_heads': 8, 'use_query_rpe_similarity': True, 'use_key_rpe_similarity': True, 'use_content_similarity': True, 'retrieve_value_rpe': True, 'retrieve_value_content': True, 'initialization_std_for_query_key_rpe': 1.0, 'initialization_std_for_value_rpe': 1.0, 'self_attention_activation': 'softmax'}, 'dual_path_transformer_layer_config': {'num_heads': 8, 'bottleneck_expansion': 2, 'key_expansion': 1, 'value_expansion': 2, 'feed_forward_network_channels': 2048, 'use_memory_self_attention': True, 'use_pixel2memory_feedback_attention': True, 'transformer_activation': 'softmax'}}, 'bn_layer': functools.partial(<class 'keras.layers.normalization.batch_normalization.SyncBatchNormalization'>, momentum=0.9900000095367432, epsilon=0.0010000000474974513), 'conv_kernel_weight_decay': 0.0}
I0913 06:02:49.119212 140486677665600 deeplab.py:96] Setting pooling size to (33, 33)
I0913 06:02:49.119329 140486677665600 aspp.py:141] Global average pooling in the ASPP pooling layer was replaced with tiled average pooling using the provided pool_size. Please make sure this behavior is intended.
I0913 06:02:51.992586 140486677665600 controller.py:393] restoring or initializing model...
WARNING:tensorflow:From /home/titanas/anaconda3/envs/alex_deeplab/lib/python3.8/site-packages/tensorflow/python/training/tracking/util.py:1343: NameBasedSaverStatus.__init__ (from tensorflow.python.training.tracking.util) is deprecated and will be removed in a future version.
Instructions for updating:
Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.
W0913 06:02:52.007497 140486677665600 deprecation.py:341] From /home/titanas/anaconda3/envs/alex_deeplab/lib/python3.8/site-packages/tensorflow/python/training/tracking/util.py:1343: NameBasedSaverStatus.__init__ (from tensorflow.python.training.tracking.util) is deprecated and will be removed in a future version.
Instructions for updating:
Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.
I0913 06:02:52.018616 140486677665600 controller.py:399] initialized model.
I0913 06:02:52.630939 140486677665600 api.py:447] Eval with scales ListWrapper([1.0])
I0913 06:02:53.491406 140486677665600 api.py:447] Global average pooling in the ASPP pooling layer was replaced with tiled average pooling using the provided pool_size. Please make sure this behavior is intended.
I0913 06:02:53.509649 140486677665600 api.py:447] Eval scale 1.0; setting pooling size to [33, 33]
I0913 06:02:56.390511 140486677665600 api.py:447] Global average pooling in the ASPP pooling layer was replaced with tiled average pooling using the provided pool_size. Please make sure this behavior is intended.
I0913 06:02:56.747714 140486677665600 controller.py:495] saved checkpoint to results/exp_001/ckpt-0.
I0913 06:02:56.748276 140486677665600 controller.py:241] train | step:      0 | training until step 5000...
2022-09-13 06:03:15.231306: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8500
I0913 06:04:47.771844 140486677665600 controller.py:466] train | step:    100 | steps/sec:    0.9 | output: 
    {'learning_rate': 0.0004954978,
     'losses/train_semantic_loss': 1.0971427,
     'losses/train_total_loss': 1.0971427}
I0913 06:06:18.399332 140486677665600 controller.py:466] train | step:    200 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00049099093,
     'losses/train_semantic_loss': 0.9997007,
     'losses/train_total_loss': 0.9997007}
I0913 06:07:49.147479 140486677665600 controller.py:466] train | step:    300 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00048647955,
     'losses/train_semantic_loss': 0.94178545,
     'losses/train_total_loss': 0.94178545}
I0913 06:09:20.792921 140486677665600 controller.py:466] train | step:    400 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00048196348,
     'losses/train_semantic_loss': 0.8972347,
     'losses/train_total_loss': 0.8972347}
I0913 06:10:52.394597 140486677665600 controller.py:466] train | step:    500 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.0004774427,
     'losses/train_semantic_loss': 0.86357147,
     'losses/train_total_loss': 0.86357147}
I0913 06:12:24.024906 140486677665600 controller.py:466] train | step:    600 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.0004729172,
     'losses/train_semantic_loss': 0.84935236,
     'losses/train_total_loss': 0.84935236}
I0913 06:13:55.175584 140486677665600 controller.py:466] train | step:    700 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00046838683,
     'losses/train_semantic_loss': 0.83793974,
     'losses/train_total_loss': 0.83793974}
I0913 06:15:26.415935 140486677665600 controller.py:466] train | step:    800 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.0004638516,
     'losses/train_semantic_loss': 0.82557076,
     'losses/train_total_loss': 0.82557076}
I0913 06:16:57.783080 140486677665600 controller.py:466] train | step:    900 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00045931144,
     'losses/train_semantic_loss': 0.7969963,
     'losses/train_total_loss': 0.7969963}
I0913 06:18:29.340494 140486677665600 controller.py:466] train | step:   1000 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.0004547663,
     'losses/train_semantic_loss': 0.80849105,
     'losses/train_total_loss': 0.80849105}
I0913 06:18:30.150287 140486677665600 controller.py:495] saved checkpoint to results/exp_001/ckpt-1000.
I0913 06:20:01.343069 140486677665600 controller.py:466] train | step:   1100 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00045021612,
     'losses/train_semantic_loss': 0.7928843,
     'losses/train_total_loss': 0.7928843}
I0913 06:21:32.678939 140486677665600 controller.py:466] train | step:   1200 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00044566081,
     'losses/train_semantic_loss': 0.7756469,
     'losses/train_total_loss': 0.7756469}
I0913 06:23:03.781825 140486677665600 controller.py:466] train | step:   1300 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00044110027,
     'losses/train_semantic_loss': 0.77103007,
     'losses/train_total_loss': 0.77103007}
I0913 06:24:35.251489 140486677665600 controller.py:466] train | step:   1400 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00043653455,
     'losses/train_semantic_loss': 0.7659806,
     'losses/train_total_loss': 0.7659806}
I0913 06:26:06.410008 140486677665600 controller.py:466] train | step:   1500 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.0004319635,
     'losses/train_semantic_loss': 0.76046145,
     'losses/train_total_loss': 0.76046145}
I0913 06:27:37.632860 140486677665600 controller.py:466] train | step:   1600 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00042738707,
     'losses/train_semantic_loss': 0.7643351,
     'losses/train_total_loss': 0.7643351}
I0913 06:29:08.570913 140486677665600 controller.py:466] train | step:   1700 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00042280517,
     'losses/train_semantic_loss': 0.7537177,
     'losses/train_total_loss': 0.7537177}
I0913 06:30:39.456781 140486677665600 controller.py:466] train | step:   1800 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00041821777,
     'losses/train_semantic_loss': 0.7282759,
     'losses/train_total_loss': 0.7282759}
I0913 06:32:10.724246 140486677665600 controller.py:466] train | step:   1900 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00041362477,
     'losses/train_semantic_loss': 0.7347085,
     'losses/train_total_loss': 0.7347085}
I0913 06:33:41.850664 140486677665600 controller.py:466] train | step:   2000 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00040902608,
     'losses/train_semantic_loss': 0.73541695,
     'losses/train_total_loss': 0.73541695}
I0913 06:33:42.674308 140486677665600 controller.py:495] saved checkpoint to results/exp_001/ckpt-2000.
I0913 06:35:13.658383 140486677665600 controller.py:466] train | step:   2100 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00040442165,
     'losses/train_semantic_loss': 0.7520546,
     'losses/train_total_loss': 0.7520546}
I0913 06:36:45.328638 140486677665600 controller.py:466] train | step:   2200 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00039981137,
     'losses/train_semantic_loss': 0.7287725,
     'losses/train_total_loss': 0.7287725}
I0913 06:38:16.672290 140486677665600 controller.py:466] train | step:   2300 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.0003951952,
     'losses/train_semantic_loss': 0.72990155,
     'losses/train_total_loss': 0.72990155}
I0913 06:39:48.158644 140486677665600 controller.py:466] train | step:   2400 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00039057303,
     'losses/train_semantic_loss': 0.6998552,
     'losses/train_total_loss': 0.6998552}
I0913 06:41:19.430730 140486677665600 controller.py:466] train | step:   2500 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00038594473,
     'losses/train_semantic_loss': 0.72399306,
     'losses/train_total_loss': 0.72399306}
I0913 06:42:50.763258 140486677665600 controller.py:466] train | step:   2600 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00038131032,
     'losses/train_semantic_loss': 0.7092882,
     'losses/train_total_loss': 0.7092882}
I0913 06:44:22.311805 140486677665600 controller.py:466] train | step:   2700 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00037666963,
     'losses/train_semantic_loss': 0.693577,
     'losses/train_total_loss': 0.693577}
I0913 06:45:53.599613 140486677665600 controller.py:466] train | step:   2800 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00037202256,
     'losses/train_semantic_loss': 0.7088667,
     'losses/train_total_loss': 0.7088667}
I0913 06:47:25.163319 140486677665600 controller.py:466] train | step:   2900 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00036736907,
     'losses/train_semantic_loss': 0.7116068,
     'losses/train_total_loss': 0.7116068}
I0913 06:48:56.698282 140486677665600 controller.py:466] train | step:   3000 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00036270893,
     'losses/train_semantic_loss': 0.6970405,
     'losses/train_total_loss': 0.6970405}
I0913 06:48:57.463970 140486677665600 controller.py:495] saved checkpoint to results/exp_001/ckpt-3000.
I0913 06:50:28.768334 140486677665600 controller.py:466] train | step:   3100 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00035804222,
     'losses/train_semantic_loss': 0.70303214,
     'losses/train_total_loss': 0.70303214}
I0913 06:51:59.917232 140486677665600 controller.py:466] train | step:   3200 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.0003533687,
     'losses/train_semantic_loss': 0.6764758,
     'losses/train_total_loss': 0.6764758}
I0913 06:53:31.235255 140486677665600 controller.py:466] train | step:   3300 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00034868822,
     'losses/train_semantic_loss': 0.6776884,
     'losses/train_total_loss': 0.6776884}
I0913 06:55:02.812642 140486677665600 controller.py:466] train | step:   3400 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00034400087,
     'losses/train_semantic_loss': 0.69349307,
     'losses/train_total_loss': 0.69349307}
I0913 06:56:34.732774 140486677665600 controller.py:466] train | step:   3500 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00033930637,
     'losses/train_semantic_loss': 0.67532814,
     'losses/train_total_loss': 0.67532814}
I0913 06:58:05.922291 140486677665600 controller.py:466] train | step:   3600 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00033460467,
     'losses/train_semantic_loss': 0.66876966,
     'losses/train_total_loss': 0.66876966}
I0913 06:59:36.998548 140486677665600 controller.py:466] train | step:   3700 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.0003298956,
     'losses/train_semantic_loss': 0.6779205,
     'losses/train_total_loss': 0.6779205}
I0913 07:01:09.007732 140486677665600 controller.py:466] train | step:   3800 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00032517905,
     'losses/train_semantic_loss': 0.67125565,
     'losses/train_total_loss': 0.67125565}
I0913 07:02:40.455068 140486677665600 controller.py:466] train | step:   3900 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00032045488,
     'losses/train_semantic_loss': 0.6807354,
     'losses/train_total_loss': 0.6807354}
I0913 07:04:11.645132 140486677665600 controller.py:466] train | step:   4000 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00031572295,
     'losses/train_semantic_loss': 0.6758322,
     'losses/train_total_loss': 0.6758322}
I0913 07:04:12.370681 140486677665600 controller.py:495] saved checkpoint to results/exp_001/ckpt-4000.
I0913 07:05:43.749514 140486677665600 controller.py:466] train | step:   4100 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00031098313,
     'losses/train_semantic_loss': 0.67034185,
     'losses/train_total_loss': 0.67034185}
I0913 07:07:15.455413 140486677665600 controller.py:466] train | step:   4200 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.0003062353,
     'losses/train_semantic_loss': 0.6758827,
     'losses/train_total_loss': 0.6758827}
I0913 07:08:47.089563 140486677665600 controller.py:466] train | step:   4300 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00030147925,
     'losses/train_semantic_loss': 0.6605729,
     'losses/train_total_loss': 0.6605729}
I0913 07:10:18.759983 140486677665600 controller.py:466] train | step:   4400 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00029671483,
     'losses/train_semantic_loss': 0.6473505,
     'losses/train_total_loss': 0.6473505}
I0913 07:11:50.382875 140486677665600 controller.py:466] train | step:   4500 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.0002919419,
     'losses/train_semantic_loss': 0.6587571,
     'losses/train_total_loss': 0.6587571}
restoring or initializing model...
initialized model.
saved checkpoint to results/exp_001/ckpt-0.
train | step:      0 | training until step 5000...
train | step:    100 | steps/sec:    0.9 | output: 
    {'learning_rate': 0.0004954978,
     'losses/train_semantic_loss': 1.0971427,
     'losses/train_total_loss': 1.0971427}
train | step:    200 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00049099093,
     'losses/train_semantic_loss': 0.9997007,
     'losses/train_total_loss': 0.9997007}
train | step:    300 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00048647955,
     'losses/train_semantic_loss': 0.94178545,
     'losses/train_total_loss': 0.94178545}
train | step:    400 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00048196348,
     'losses/train_semantic_loss': 0.8972347,
     'losses/train_total_loss': 0.8972347}
train | step:    500 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.0004774427,
     'losses/train_semantic_loss': 0.86357147,
     'losses/train_total_loss': 0.86357147}
train | step:    600 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.0004729172,
     'losses/train_semantic_loss': 0.84935236,
     'losses/train_total_loss': 0.84935236}
train | step:    700 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00046838683,
     'losses/train_semantic_loss': 0.83793974,
     'losses/train_total_loss': 0.83793974}
train | step:    800 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.0004638516,
     'losses/train_semantic_loss': 0.82557076,
     'losses/train_total_loss': 0.82557076}
train | step:    900 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00045931144,
     'losses/train_semantic_loss': 0.7969963,
     'losses/train_total_loss': 0.7969963}
train | step:   1000 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.0004547663,
     'losses/train_semantic_loss': 0.80849105,
     'losses/train_total_loss': 0.80849105}
saved checkpoint to results/exp_001/ckpt-1000.
train | step:   1100 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00045021612,
     'losses/train_semantic_loss': 0.7928843,
     'losses/train_total_loss': 0.7928843}
train | step:   1200 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00044566081,
     'losses/train_semantic_loss': 0.7756469,
     'losses/train_total_loss': 0.7756469}
train | step:   1300 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00044110027,
     'losses/train_semantic_loss': 0.77103007,
     'losses/train_total_loss': 0.77103007}
train | step:   1400 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00043653455,
     'losses/train_semantic_loss': 0.7659806,
     'losses/train_total_loss': 0.7659806}
train | step:   1500 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.0004319635,
     'losses/train_semantic_loss': 0.76046145,
     'losses/train_total_loss': 0.76046145}
train | step:   1600 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00042738707,
     'losses/train_semantic_loss': 0.7643351,
     'losses/train_total_loss': 0.7643351}
train | step:   1700 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00042280517,
     'losses/train_semantic_loss': 0.7537177,
     'losses/train_total_loss': 0.7537177}
train | step:   1800 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00041821777,
     'losses/train_semantic_loss': 0.7282759,
     'losses/train_total_loss': 0.7282759}
train | step:   1900 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00041362477,
     'losses/train_semantic_loss': 0.7347085,
     'losses/train_total_loss': 0.7347085}
train | step:   2000 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00040902608,
     'losses/train_semantic_loss': 0.73541695,
     'losses/train_total_loss': 0.73541695}
saved checkpoint to results/exp_001/ckpt-2000.
train | step:   2100 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00040442165,
     'losses/train_semantic_loss': 0.7520546,
     'losses/train_total_loss': 0.7520546}
train | step:   2200 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00039981137,
     'losses/train_semantic_loss': 0.7287725,
     'losses/train_total_loss': 0.7287725}
train | step:   2300 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.0003951952,
     'losses/train_semantic_loss': 0.72990155,
     'losses/train_total_loss': 0.72990155}
train | step:   2400 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00039057303,
     'losses/train_semantic_loss': 0.6998552,
     'losses/train_total_loss': 0.6998552}
train | step:   2500 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00038594473,
     'losses/train_semantic_loss': 0.72399306,
     'losses/train_total_loss': 0.72399306}
train | step:   2600 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00038131032,
     'losses/train_semantic_loss': 0.7092882,
     'losses/train_total_loss': 0.7092882}
train | step:   2700 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00037666963,
     'losses/train_semantic_loss': 0.693577,
     'losses/train_total_loss': 0.693577}
train | step:   2800 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00037202256,
     'losses/train_semantic_loss': 0.7088667,
     'losses/train_total_loss': 0.7088667}
train | step:   2900 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00036736907,
     'losses/train_semantic_loss': 0.7116068,
     'losses/train_total_loss': 0.7116068}
train | step:   3000 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00036270893,
     'losses/train_semantic_loss': 0.6970405,
     'losses/train_total_loss': 0.6970405}
saved checkpoint to results/exp_001/ckpt-3000.
train | step:   3100 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00035804222,
     'losses/train_semantic_loss': 0.70303214,
     'losses/train_total_loss': 0.70303214}
train | step:   3200 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.0003533687,
     'losses/train_semantic_loss': 0.6764758,
     'losses/train_total_loss': 0.6764758}
train | step:   3300 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00034868822,
     'losses/train_semantic_loss': 0.6776884,
     'losses/train_total_loss': 0.6776884}
train | step:   3400 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00034400087,
     'losses/train_semantic_loss': 0.69349307,
     'losses/train_total_loss': 0.69349307}
train | step:   3500 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00033930637,
     'losses/train_semantic_loss': 0.67532814,
     'losses/train_total_loss': 0.67532814}
train | step:   3600 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00033460467,
     'losses/train_semantic_loss': 0.66876966,
     'losses/train_total_loss': 0.66876966}
train | step:   3700 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.0003298956,
     'losses/train_semantic_loss': 0.6779205,
     'losses/train_total_loss': 0.6779205}
train | step:   3800 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00032517905,
     'losses/train_semantic_loss': 0.67125565,
     'losses/train_total_loss': 0.67125565}
train | step:   3900 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00032045488,
     'losses/train_semantic_loss': 0.6807354,
     'losses/train_total_loss': 0.6807354}
train | step:   4000 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00031572295,
     'losses/train_semantic_loss': 0.6758322,
     'losses/train_total_loss': 0.6758322}
saved checkpoint to results/exp_001/ckpt-4000.
train | step:   4100 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00031098313,
     'losses/train_semantic_loss': 0.67034185,
     'losses/train_total_loss': 0.67034185}
train | step:   4200 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.0003062353,
     'losses/train_semantic_loss': 0.6758827,
     'losses/train_total_loss': 0.6758827}
train | step:   4300 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00030147925,
     'losses/train_semantic_loss': 0.6605729,
     'losses/train_total_loss': 0.6605729}
train | step:   4400 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00029671483,
     'losses/train_semantic_loss': 0.6473505,
     'losses/train_total_loss': 0.6473505}
train | step:   4500 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.0002919419,
     'losses/train_semantic_loss': 0.6587571,
     'losses/train_total_loss': 0.6587571}I0913 07:13:22.176740 140486677665600 controller.py:466] train | step:   4600 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00028716028,
     'losses/train_semantic_loss': 0.6589973,
     'losses/train_total_loss': 0.6589973}
I0913 07:14:53.848088 140486677665600 controller.py:466] train | step:   4700 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00028236985,
     'losses/train_semantic_loss': 0.6389558,
     'losses/train_total_loss': 0.6389558}
I0913 07:16:25.213357 140486677665600 controller.py:466] train | step:   4800 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00027757033,
     'losses/train_semantic_loss': 0.62851655,
     'losses/train_total_loss': 0.62851655}
I0913 07:17:56.558385 140486677665600 controller.py:466] train | step:   4900 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00027276156,
     'losses/train_semantic_loss': 0.66500604,
     'losses/train_total_loss': 0.66500604}
I0913 07:19:27.788765 140486677665600 controller.py:466] train | step:   5000 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.0002679434,
     'losses/train_semantic_loss': 0.6268103,
     'losses/train_total_loss': 0.6268103}
I0913 07:19:28.667962 140486677665600 controller.py:495] saved checkpoint to results/exp_001/ckpt-5000.
I0913 07:19:28.668533 140486677665600 controller.py:282]  eval | step:   5000 | running complete evaluation...
I0913 07:19:28.996433 140486677665600 api.py:447] Eval with scales ListWrapper([1.0])
I0913 07:19:29.024826 140486677665600 api.py:447] Global average pooling in the ASPP pooling layer was replaced with tiled average pooling using the provided pool_size. Please make sure this behavior is intended.
I0913 07:19:29.049212 140486677665600 api.py:447] Eval scale 1.0; setting pooling size to [33, 33]
I0913 07:19:29.670651 140486677665600 api.py:447] Global average pooling in the ASPP pooling layer was replaced with tiled average pooling using the provided pool_size. Please make sure this behavior is intended.
I0913 07:19:43.581376 140486677665600 loop_fns.py:81] The dataset iterator is exhausted after 100 steps.
I0913 07:19:43.597588 140486677665600 controller.py:295]  eval | step:   5000 | eval time:   14.9 sec | output: 
    {'evaluation/iou/IoU': 0.61729276,
     'losses/eval_semantic_loss': 0.71943223,
     'losses/eval_total_loss': 0.71943223}
I0913 07:19:43.603792 140486677665600 controller.py:241] train | step:   5000 | training until step 10000...
I0913 07:21:14.610280 140486677665600 controller.py:466] train | step:   5100 | steps/sec:    0.9 | output: 
    {'learning_rate': 0.00026311554,
     'losses/train_semantic_loss': 0.6388172,
     'losses/train_total_loss': 0.6388172}
I0913 07:22:46.095300 140486677665600 controller.py:466] train | step:   5200 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00025827784,
     'losses/train_semantic_loss': 0.6084909,
     'losses/train_total_loss': 0.6084909}
I0913 07:24:17.352902 140486677665600 controller.py:466] train | step:   5300 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00025343004,
     'losses/train_semantic_loss': 0.64326155,
     'losses/train_total_loss': 0.64326155}
I0913 07:25:48.754640 140486677665600 controller.py:466] train | step:   5400 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00024857192,
     'losses/train_semantic_loss': 0.6204016,
     'losses/train_total_loss': 0.6204016}
I0913 07:27:20.130034 140486677665600 controller.py:466] train | step:   5500 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00024370321,
     'losses/train_semantic_loss': 0.63030833,
     'losses/train_total_loss': 0.63030833}
I0913 07:28:51.798676 140486677665600 controller.py:466] train | step:   5600 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.0002388237,
     'losses/train_semantic_loss': 0.62896144,
     'losses/train_total_loss': 0.62896144}
I0913 07:30:23.266549 140486677665600 controller.py:466] train | step:   5700 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00023393307,
     'losses/train_semantic_loss': 0.63182503,
     'losses/train_total_loss': 0.63182503}
I0913 07:31:54.729683 140486677665600 controller.py:466] train | step:   5800 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00022903107,
     'losses/train_semantic_loss': 0.62155455,
     'losses/train_total_loss': 0.62155455}
I0913 07:33:26.015321 140486677665600 controller.py:466] train | step:   5900 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00022411738,
     'losses/train_semantic_loss': 0.6041171,
     'losses/train_total_loss': 0.6041171}
I0913 07:34:57.171636 140486677665600 controller.py:466] train | step:   6000 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00021919166,
     'losses/train_semantic_loss': 0.5862693,
     'losses/train_total_loss': 0.5862693}
I0913 07:34:57.890723 140486677665600 controller.py:495] saved checkpoint to results/exp_001/ckpt-6000.
I0913 07:36:29.562953 140486677665600 controller.py:466] train | step:   6100 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00021425361,
     'losses/train_semantic_loss': 0.6185944,
     'losses/train_total_loss': 0.6185944}
I0913 07:38:01.295146 140486677665600 controller.py:466] train | step:   6200 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00020930292,
     'losses/train_semantic_loss': 0.6090601,
     'losses/train_total_loss': 0.6090601}
I0913 07:39:32.760748 140486677665600 controller.py:466] train | step:   6300 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00020433914,
     'losses/train_semantic_loss': 0.6026183,
     'losses/train_total_loss': 0.6026183}
I0913 07:41:04.335902 140486677665600 controller.py:466] train | step:   6400 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00019936197,
     'losses/train_semantic_loss': 0.58776003,
     'losses/train_total_loss': 0.58776003}
I0913 07:42:36.004064 140486677665600 controller.py:466] train | step:   6500 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00019437094,
     'losses/train_semantic_loss': 0.6259824,
     'losses/train_total_loss': 0.6259824}
I0913 07:44:07.444312 140486677665600 controller.py:466] train | step:   6600 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00018936557,
     'losses/train_semantic_loss': 0.6042295,
     'losses/train_total_loss': 0.6042295}
I0913 07:45:38.970529 140486677665600 controller.py:466] train | step:   6700 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00018434551,
     'losses/train_semantic_loss': 0.58202904,
     'losses/train_total_loss': 0.58202904}
I0913 07:47:10.345489 140486677665600 controller.py:466] train | step:   6800 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.0001793102,
     'losses/train_semantic_loss': 0.5927137,
     'losses/train_total_loss': 0.5927137}
I0913 07:48:41.876047 140486677665600 controller.py:466] train | step:   6900 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00017425913,
     'losses/train_semantic_loss': 0.58970916,
     'losses/train_total_loss': 0.58970916}
I0913 07:50:13.880238 140486677665600 controller.py:466] train | step:   7000 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00016919174,
     'losses/train_semantic_loss': 0.59415513,
     'losses/train_total_loss': 0.59415513}
I0913 07:50:14.722424 140486677665600 controller.py:495] saved checkpoint to results/exp_001/ckpt-7000.
I0913 07:51:46.028711 140486677665600 controller.py:466] train | step:   7100 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00016410745,
     'losses/train_semantic_loss': 0.58710486,
     'losses/train_total_loss': 0.58710486}
I0913 07:53:17.948450 140486677665600 controller.py:466] train | step:   7200 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00015900553,
     'losses/train_semantic_loss': 0.57606983,
     'losses/train_total_loss': 0.57606983}
I0913 07:54:49.877607 140486677665600 controller.py:466] train | step:   7300 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00015388538,
     'losses/train_semantic_loss': 0.58922315,
     'losses/train_total_loss': 0.58922315}
I0913 07:56:21.213288 140486677665600 controller.py:466] train | step:   7400 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00014874624,
     'losses/train_semantic_loss': 0.579593,
     'losses/train_total_loss': 0.579593}
I0913 07:57:52.821998 140486677665600 controller.py:466] train | step:   7500 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00014358731,
     'losses/train_semantic_loss': 0.56899667,
     'losses/train_total_loss': 0.56899667}
I0913 07:59:24.285592 140486677665600 controller.py:466] train | step:   7600 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00013840769,
     'losses/train_semantic_loss': 0.58099306,
     'losses/train_total_loss': 0.58099306}
I0913 08:00:55.914264 140486677665600 controller.py:466] train | step:   7700 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.0001332064,
     'losses/train_semantic_loss': 0.5898137,
     'losses/train_total_loss': 0.5898137}
I0913 08:02:27.467774 140486677665600 controller.py:466] train | step:   7800 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00012798246,
     'losses/train_semantic_loss': 0.5532005,
     'losses/train_total_loss': 0.5532005}
I0913 08:03:59.198809 140486677665600 controller.py:466] train | step:   7900 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.0001227347,
     'losses/train_semantic_loss': 0.56635356,
     'losses/train_total_loss': 0.56635356}
I0913 08:05:30.836535 140486677665600 controller.py:466] train | step:   8000 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00011746189,
     'losses/train_semantic_loss': 0.548403,
     'losses/train_total_loss': 0.548403}
I0913 08:05:31.731983 140486677665600 controller.py:495] saved checkpoint to results/exp_001/ckpt-8000.
I0913 08:07:03.249536 140486677665600 controller.py:466] train | step:   8100 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00011216264,
     'losses/train_semantic_loss': 0.5613165,
     'losses/train_total_loss': 0.5613165}
I0913 08:08:34.778494 140486677665600 controller.py:466] train | step:   8200 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00010683544,
     'losses/train_semantic_loss': 0.5547561,
     'losses/train_total_loss': 0.5547561}
I0913 08:10:06.460154 140486677665600 controller.py:466] train | step:   8300 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00010147853,
     'losses/train_semantic_loss': 0.5499042,
     'losses/train_total_loss': 0.5499042}
I0913 08:11:38.205124 140486677665600 controller.py:466] train | step:   8400 | steps/sec:    1.1 | output: 
    {'learning_rate': 9.608998e-05,
     'losses/train_semantic_loss': 0.55712503,
     'losses/train_total_loss': 0.55712503}
I0913 08:13:09.553917 140486677665600 controller.py:466] train | step:   8500 | steps/sec:    1.1 | output: 
    {'learning_rate': 9.066759e-05,
     'losses/train_semantic_loss': 0.57595766,
     'losses/train_total_loss': 0.57595766}
I0913 08:14:41.216255 140486677665600 controller.py:466] train | step:   8600 | steps/sec:    1.1 | output: 
    {'learning_rate': 8.520896e-05,
     'losses/train_semantic_loss': 0.5410403,
     'losses/train_total_loss': 0.5410403}
I0913 08:16:12.801285 140486677665600 controller.py:466] train | step:   8700 | steps/sec:    1.1 | output: 
    {'learning_rate': 7.971114e-05,
     'losses/train_semantic_loss': 0.5694424,
     'losses/train_total_loss': 0.5694424}
I0913 08:17:44.202733 140486677665600 controller.py:466] train | step:   8800 | steps/sec:    1.1 | output: 
    {'learning_rate': 7.417084e-05,
     'losses/train_semantic_loss': 0.53795594,
     'losses/train_total_loss': 0.53795594}
I0913 08:19:15.529464 140486677665600 controller.py:466] train | step:   8900 | steps/sec:    1.1 | output: 
    {'learning_rate': 6.858411e-05,
     'losses/train_semantic_loss': 0.5427251,
     'losses/train_total_loss': 0.5427251}

train | step:   4600 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00028716028,
     'losses/train_semantic_loss': 0.6589973,
     'losses/train_total_loss': 0.6589973}
train | step:   4700 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00028236985,
     'losses/train_semantic_loss': 0.6389558,
     'losses/train_total_loss': 0.6389558}
train | step:   4800 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00027757033,
     'losses/train_semantic_loss': 0.62851655,
     'losses/train_total_loss': 0.62851655}
train | step:   4900 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00027276156,
     'losses/train_semantic_loss': 0.66500604,
     'losses/train_total_loss': 0.66500604}
train | step:   5000 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.0002679434,
     'losses/train_semantic_loss': 0.6268103,
     'losses/train_total_loss': 0.6268103}
saved checkpoint to results/exp_001/ckpt-5000.
 eval | step:   5000 | running complete evaluation...
 eval | step:   5000 | eval time:   14.9 sec | output: 
    {'evaluation/iou/IoU': 0.61729276,
     'losses/eval_semantic_loss': 0.71943223,
     'losses/eval_total_loss': 0.71943223}
train | step:   5000 | training until step 10000...
train | step:   5100 | steps/sec:    0.9 | output: 
    {'learning_rate': 0.00026311554,
     'losses/train_semantic_loss': 0.6388172,
     'losses/train_total_loss': 0.6388172}
train | step:   5200 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00025827784,
     'losses/train_semantic_loss': 0.6084909,
     'losses/train_total_loss': 0.6084909}
train | step:   5300 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00025343004,
     'losses/train_semantic_loss': 0.64326155,
     'losses/train_total_loss': 0.64326155}
train | step:   5400 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00024857192,
     'losses/train_semantic_loss': 0.6204016,
     'losses/train_total_loss': 0.6204016}
train | step:   5500 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00024370321,
     'losses/train_semantic_loss': 0.63030833,
     'losses/train_total_loss': 0.63030833}
train | step:   5600 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.0002388237,
     'losses/train_semantic_loss': 0.62896144,
     'losses/train_total_loss': 0.62896144}
train | step:   5700 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00023393307,
     'losses/train_semantic_loss': 0.63182503,
     'losses/train_total_loss': 0.63182503}
train | step:   5800 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00022903107,
     'losses/train_semantic_loss': 0.62155455,
     'losses/train_total_loss': 0.62155455}
train | step:   5900 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00022411738,
     'losses/train_semantic_loss': 0.6041171,
     'losses/train_total_loss': 0.6041171}
train | step:   6000 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00021919166,
     'losses/train_semantic_loss': 0.5862693,
     'losses/train_total_loss': 0.5862693}
saved checkpoint to results/exp_001/ckpt-6000.
train | step:   6100 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00021425361,
     'losses/train_semantic_loss': 0.6185944,
     'losses/train_total_loss': 0.6185944}
train | step:   6200 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00020930292,
     'losses/train_semantic_loss': 0.6090601,
     'losses/train_total_loss': 0.6090601}
train | step:   6300 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00020433914,
     'losses/train_semantic_loss': 0.6026183,
     'losses/train_total_loss': 0.6026183}
train | step:   6400 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00019936197,
     'losses/train_semantic_loss': 0.58776003,
     'losses/train_total_loss': 0.58776003}
train | step:   6500 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00019437094,
     'losses/train_semantic_loss': 0.6259824,
     'losses/train_total_loss': 0.6259824}
train | step:   6600 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00018936557,
     'losses/train_semantic_loss': 0.6042295,
     'losses/train_total_loss': 0.6042295}
train | step:   6700 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00018434551,
     'losses/train_semantic_loss': 0.58202904,
     'losses/train_total_loss': 0.58202904}
train | step:   6800 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.0001793102,
     'losses/train_semantic_loss': 0.5927137,
     'losses/train_total_loss': 0.5927137}
train | step:   6900 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00017425913,
     'losses/train_semantic_loss': 0.58970916,
     'losses/train_total_loss': 0.58970916}
train | step:   7000 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00016919174,
     'losses/train_semantic_loss': 0.59415513,
     'losses/train_total_loss': 0.59415513}
saved checkpoint to results/exp_001/ckpt-7000.
train | step:   7100 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00016410745,
     'losses/train_semantic_loss': 0.58710486,
     'losses/train_total_loss': 0.58710486}
train | step:   7200 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00015900553,
     'losses/train_semantic_loss': 0.57606983,
     'losses/train_total_loss': 0.57606983}
train | step:   7300 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00015388538,
     'losses/train_semantic_loss': 0.58922315,
     'losses/train_total_loss': 0.58922315}
train | step:   7400 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00014874624,
     'losses/train_semantic_loss': 0.579593,
     'losses/train_total_loss': 0.579593}
train | step:   7500 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00014358731,
     'losses/train_semantic_loss': 0.56899667,
     'losses/train_total_loss': 0.56899667}
train | step:   7600 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00013840769,
     'losses/train_semantic_loss': 0.58099306,
     'losses/train_total_loss': 0.58099306}
train | step:   7700 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.0001332064,
     'losses/train_semantic_loss': 0.5898137,
     'losses/train_total_loss': 0.5898137}
train | step:   7800 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00012798246,
     'losses/train_semantic_loss': 0.5532005,
     'losses/train_total_loss': 0.5532005}
train | step:   7900 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.0001227347,
     'losses/train_semantic_loss': 0.56635356,
     'losses/train_total_loss': 0.56635356}
train | step:   8000 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00011746189,
     'losses/train_semantic_loss': 0.548403,
     'losses/train_total_loss': 0.548403}
saved checkpoint to results/exp_001/ckpt-8000.
train | step:   8100 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00011216264,
     'losses/train_semantic_loss': 0.5613165,
     'losses/train_total_loss': 0.5613165}
train | step:   8200 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00010683544,
     'losses/train_semantic_loss': 0.5547561,
     'losses/train_total_loss': 0.5547561}
train | step:   8300 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.00010147853,
     'losses/train_semantic_loss': 0.5499042,
     'losses/train_total_loss': 0.5499042}
train | step:   8400 | steps/sec:    1.1 | output: 
    {'learning_rate': 9.608998e-05,
     'losses/train_semantic_loss': 0.55712503,
     'losses/train_total_loss': 0.55712503}
train | step:   8500 | steps/sec:    1.1 | output: 
    {'learning_rate': 9.066759e-05,
     'losses/train_semantic_loss': 0.57595766,
     'losses/train_total_loss': 0.57595766}
train | step:   8600 | steps/sec:    1.1 | output: 
    {'learning_rate': 8.520896e-05,
     'losses/train_semantic_loss': 0.5410403,
     'losses/train_total_loss': 0.5410403}
train | step:   8700 | steps/sec:    1.1 | output: 
    {'learning_rate': 7.971114e-05,
     'losses/train_semantic_loss': 0.5694424,
     'losses/train_total_loss': 0.5694424}
train | step:   8800 | steps/sec:    1.1 | output: 
    {'learning_rate': 7.417084e-05,
     'losses/train_semantic_loss': 0.53795594,
     'losses/train_total_loss': 0.53795594}
train | step:   8900 | steps/sec:    1.1 | output: 
    {'learning_rate': 6.858411e-05,
     'losses/train_semantic_loss': 0.5427251,
     'losses/train_total_loss': 0.5427251}I0913 08:20:46.877066 140486677665600 controller.py:466] train | step:   9000 | steps/sec:    1.1 | output: 
    {'learning_rate': 6.29463e-05,
     'losses/train_semantic_loss': 0.53205144,
     'losses/train_total_loss': 0.53205144}
I0913 08:20:47.707826 140486677665600 controller.py:495] saved checkpoint to results/exp_001/ckpt-9000.
I0913 08:22:19.037151 140486677665600 controller.py:466] train | step:   9100 | steps/sec:    1.1 | output: 
    {'learning_rate': 5.7251673e-05,
     'losses/train_semantic_loss': 0.53711545,
     'losses/train_total_loss': 0.53711545}
I0913 08:23:50.756501 140486677665600 controller.py:466] train | step:   9200 | steps/sec:    1.1 | output: 
    {'learning_rate': 5.1493324e-05,
     'losses/train_semantic_loss': 0.5415965,
     'losses/train_total_loss': 0.5415965}
I0913 08:25:22.372262 140486677665600 controller.py:466] train | step:   9300 | steps/sec:    1.1 | output: 
    {'learning_rate': 4.5662342e-05,
     'losses/train_semantic_loss': 0.54923385,
     'losses/train_total_loss': 0.54923385}
I0913 08:26:53.409047 140486677665600 controller.py:466] train | step:   9400 | steps/sec:    1.1 | output: 
    {'learning_rate': 3.9747174e-05,
     'losses/train_semantic_loss': 0.5397937,
     'losses/train_total_loss': 0.5397937}
I0913 08:28:24.790101 140486677665600 controller.py:466] train | step:   9500 | steps/sec:    1.1 | output: 
    {'learning_rate': 3.373209e-05,
     'losses/train_semantic_loss': 0.5302072,
     'losses/train_total_loss': 0.5302072}
I0913 08:29:56.021053 140486677665600 controller.py:466] train | step:   9600 | steps/sec:    1.1 | output: 
    {'learning_rate': 2.7594608e-05,
     'losses/train_semantic_loss': 0.5096406,
     'losses/train_total_loss': 0.5096406}
I0913 08:31:27.188127 140486677665600 controller.py:466] train | step:   9700 | steps/sec:    1.1 | output: 
    {'learning_rate': 2.1299958e-05,
     'losses/train_semantic_loss': 0.5411964,
     'losses/train_total_loss': 0.5411964}
I0913 08:32:58.534977 140486677665600 controller.py:466] train | step:   9800 | steps/sec:    1.1 | output: 
    {'learning_rate': 1.4787565e-05,
     'losses/train_semantic_loss': 0.5409473,
     'losses/train_total_loss': 0.5409473}
I0913 08:34:29.862924 140486677665600 controller.py:466] train | step:   9900 | steps/sec:    1.1 | output: 
    {'learning_rate': 7.924459e-06,
     'losses/train_semantic_loss': 0.52928376,
     'losses/train_total_loss': 0.52928376}
I0913 08:36:01.205585 140486677665600 controller.py:466] train | step:  10000 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.0,
     'losses/train_semantic_loss': 0.5389968,
     'losses/train_total_loss': 0.5389968}
I0913 08:36:02.020762 140486677665600 controller.py:495] saved checkpoint to results/exp_001/ckpt-10000.
I0913 08:36:02.021358 140486677665600 controller.py:282]  eval | step:  10000 | running complete evaluation...
I0913 08:36:14.857398 140486677665600 loop_fns.py:81] The dataset iterator is exhausted after 100 steps.
I0913 08:36:14.891733 140486677665600 controller.py:295]  eval | step:  10000 | eval time:   12.9 sec | output: 
    {'evaluation/iou/IoU': 0.666567,
     'losses/eval_semantic_loss': 0.6373477,
     'losses/eval_total_loss': 0.6373477}

train | step:   9000 | steps/sec:    1.1 | output: 
    {'learning_rate': 6.29463e-05,
     'losses/train_semantic_loss': 0.53205144,
     'losses/train_total_loss': 0.53205144}
saved checkpoint to results/exp_001/ckpt-9000.
train | step:   9100 | steps/sec:    1.1 | output: 
    {'learning_rate': 5.7251673e-05,
     'losses/train_semantic_loss': 0.53711545,
     'losses/train_total_loss': 0.53711545}
train | step:   9200 | steps/sec:    1.1 | output: 
    {'learning_rate': 5.1493324e-05,
     'losses/train_semantic_loss': 0.5415965,
     'losses/train_total_loss': 0.5415965}
train | step:   9300 | steps/sec:    1.1 | output: 
    {'learning_rate': 4.5662342e-05,
     'losses/train_semantic_loss': 0.54923385,
     'losses/train_total_loss': 0.54923385}
train | step:   9400 | steps/sec:    1.1 | output: 
    {'learning_rate': 3.9747174e-05,
     'losses/train_semantic_loss': 0.5397937,
     'losses/train_total_loss': 0.5397937}
train | step:   9500 | steps/sec:    1.1 | output: 
    {'learning_rate': 3.373209e-05,
     'losses/train_semantic_loss': 0.5302072,
     'losses/train_total_loss': 0.5302072}
train | step:   9600 | steps/sec:    1.1 | output: 
    {'learning_rate': 2.7594608e-05,
     'losses/train_semantic_loss': 0.5096406,
     'losses/train_total_loss': 0.5096406}
train | step:   9700 | steps/sec:    1.1 | output: 
    {'learning_rate': 2.1299958e-05,
     'losses/train_semantic_loss': 0.5411964,
     'losses/train_total_loss': 0.5411964}
train | step:   9800 | steps/sec:    1.1 | output: 
    {'learning_rate': 1.4787565e-05,
     'losses/train_semantic_loss': 0.5409473,
     'losses/train_total_loss': 0.5409473}
train | step:   9900 | steps/sec:    1.1 | output: 
    {'learning_rate': 7.924459e-06,
     'losses/train_semantic_loss': 0.52928376,
     'losses/train_total_loss': 0.52928376}
train | step:  10000 | steps/sec:    1.1 | output: 
    {'learning_rate': 0.0,
     'losses/train_semantic_loss': 0.5389968,
     'losses/train_total_loss': 0.5389968}
saved checkpoint to results/exp_001/ckpt-10000.
 eval | step:  10000 | running complete evaluation...
 eval | step:  10000 | eval time:   12.9 sec | output: 
    {'evaluation/iou/IoU': 0.666567,
     'losses/eval_semantic_loss': 0.6373477,
     'losses/eval_total_loss': 0.6373477}
